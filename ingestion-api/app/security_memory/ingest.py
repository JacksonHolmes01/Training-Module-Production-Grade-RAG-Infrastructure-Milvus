"""
Expanded VSCode Implementation ingestion entrypoint â€” Milvus version.

Run:
    docker exec -i ingestion-api python -m app.security_memory.ingest

Changes from Qdrant version:
  - _ensure_collection() creates a pymilvus Collection with typed schema.
  - _upsert() uses collection.insert() (columnar) + collection.flush().
  - IDs are auto-generated by Milvus (auto_id=True on the schema).
  - tags are JSON-encoded into a VARCHAR field.

Requirements:
  - MILVUS_HOST / MILVUS_PORT point to the milvus service
  - OLLAMA_BASE_URL points to ollama service
  - SECURITY_EMBED_DIM matches the embedding model (nomic-embed-text = 768)
"""

import os
import re
import json
from pathlib import Path
from typing import List, Dict, Any

import httpx
from pymilvus import (
    connections,
    utility,
    Collection,
    CollectionSchema,
    FieldSchema,
    DataType,
)

# ----------------------------- #
# Config                        #
# ----------------------------- #
MILVUS_HOST    = os.getenv("MILVUS_HOST", "milvus")
MILVUS_PORT    = int(os.getenv("MILVUS_PORT", "19530"))
OLLAMA_BASE_URL      = os.getenv("OLLAMA_BASE_URL", "http://ollama:11434").rstrip("/")
OLLAMA_EMBED_MODEL   = os.getenv("OLLAMA_EMBED_MODEL", "nomic-embed-text")
SECURITY_COLLECTION  = os.getenv("SECURITY_COLLECTION", "ExpandedVSCodeMemory")
SECURITY_CHUNK_CHARS = int(os.getenv("SECURITY_CHUNK_CHARS", "1200"))
SECURITY_CHUNK_OVERLAP = int(os.getenv("SECURITY_CHUNK_OVERLAP", "200"))
_raw_dim     = (os.getenv("SECURITY_EMBED_DIM") or "").strip()
SECURITY_EMBED_DIM = int(_raw_dim) if _raw_dim else 768
DATA_DIR     = Path(os.getenv("SECURITY_DATA_DIR", "/securitymemory/data"))

TAG_KEYS = [
    "nist", "cis", "mitre", "owasp", "docker", "kubernetes",
    "linux", "cloud", "iam", "sdlc", "appsec", "containers"
]

# ----------------------------- #
# Text helpers                  #
# ----------------------------- #

def _read_text(path: Path) -> str:
    return path.read_text(encoding="utf-8", errors="ignore")

def _guess_tags(path: Path) -> List[str]:
    s = str(path.as_posix()).lower()
    return sorted({k for k in TAG_KEYS if k in s})

def _normalize(text: str) -> str:
    text = text.replace("\r\n", "\n")
    text = re.sub(r"\n{3,}", "\n\n", text).strip()
    return text

def _chunk(text: str, chunk_chars: int, overlap: int) -> List[str]:
    text = _normalize(text)
    if not text:
        return []
    out, start, n = [], 0, len(text)
    while start < n:
        end = min(n, start + chunk_chars)
        piece = text[start:end].strip()
        if piece:
            out.append(piece)
        if end == n:
            break
        start = max(0, end - overlap)
    return out


# ----------------------------- #
# Embeddings (Ollama)           #
# ----------------------------- #

async def _embed(texts: List[str]) -> List[List[float]]:
    out: List[List[float]] = []
    timeout = httpx.Timeout(180.0, connect=30.0)
    async with httpx.AsyncClient(timeout=timeout) as client:
        for t in texts:
            r = await client.post(
                f"{OLLAMA_BASE_URL}/api/embeddings",
                json={"model": OLLAMA_EMBED_MODEL, "prompt": t},
            )
            if r.status_code >= 400:
                raise RuntimeError(f"Ollama embeddings failed: {r.status_code} {r.text}")
            data = r.json()
            emb  = data.get("embedding")
            if not emb:
                raise RuntimeError(f"Unexpected Ollama embeddings response: {data}")
            if len(emb) != SECURITY_EMBED_DIM:
                raise RuntimeError(
                    f"Embedding dim mismatch: got {len(emb)} but "
                    f"SECURITY_EMBED_DIM={SECURITY_EMBED_DIM}. "
                    "Fix SECURITY_EMBED_DIM or change the embedding model."
                )
            out.append(emb)
    return out


# ----------------------------- #
# Milvus collection + upsert    #
# ----------------------------- #

def _connect() -> None:
    connections.connect(alias="default", host=MILVUS_HOST, port=MILVUS_PORT, timeout=30)

def _build_schema() -> CollectionSchema:
    return CollectionSchema(
        fields=[
            FieldSchema(name="id",          dtype=DataType.INT64,        is_primary=True, auto_id=True),
            FieldSchema(name="embedding",   dtype=DataType.FLOAT_VECTOR, dim=SECURITY_EMBED_DIM),
            FieldSchema(name="text",        dtype=DataType.VARCHAR,       max_length=65535),
            FieldSchema(name="title",       dtype=DataType.VARCHAR,       max_length=1024),
            FieldSchema(name="source",      dtype=DataType.VARCHAR,       max_length=512),
            FieldSchema(name="tags",        dtype=DataType.VARCHAR,       max_length=2048),
            FieldSchema(name="chunk_index", dtype=DataType.INT64),
            FieldSchema(name="doc_path",    dtype=DataType.VARCHAR,       max_length=2048),
        ],
        description="Security memory corpus",
    )

async def _ensure_collection() -> Collection:
    _connect()
    if not utility.has_collection(SECURITY_COLLECTION):
        schema = _build_schema()
        col    = Collection(name=SECURITY_COLLECTION, schema=schema)
        col.create_index(
            field_name="embedding",
            index_params={
                "metric_type": "COSINE",
                "index_type":  "HNSW",
                "params":      {"M": 16, "efConstruction": 200},
            },
        )
        print(f"[security-memory] Created Milvus collection: {SECURITY_COLLECTION}")
    else:
        col = Collection(name=SECURITY_COLLECTION)
    col.load()
    return col

async def _upsert(col: Collection, batch_meta: List[Dict], vecs: List[List[float]]) -> None:
    """
    Milvus columnar insert.  Field order must match schema field order
    (excluding auto-id field 'id').
    """
    data = [
        vecs,                                                # embedding
        [m["text"][:65535]            for m in batch_meta], # text
        [(m["title"] or "")[:1024]    for m in batch_meta], # title
        [(m["source"] or "")[:512]    for m in batch_meta], # source
        [json.dumps(m["tags"])[:2048] for m in batch_meta], # tags (JSON)
        [m["chunk_index"]             for m in batch_meta], # chunk_index
        [(m["doc_path"] or "")[:2048] for m in batch_meta], # doc_path
    ]
    col.insert(data)
    col.flush()  # persist to storage


# ----------------------------- #
# Main                          #
# ----------------------------- #

async def main() -> None:
    if not DATA_DIR.exists():
        raise SystemExit(f"SECURITY_DATA_DIR not found: {DATA_DIR}")

    col = await _ensure_collection()

    files = [
        p for p in DATA_DIR.rglob("*")
        if p.is_file() and p.suffix.lower() in {".md", ".txt"}
    ]
    if not files:
        raise SystemExit(f"No .md/.txt files found under {DATA_DIR}. Add docs and retry.")

    BATCH = 32
    batch_texts: List[str]        = []
    batch_meta:  List[Dict]       = []
    total_chunks = 0

    for fp in files:
        raw    = _read_text(fp)
        chunks = _chunk(raw, SECURITY_CHUNK_CHARS, SECURITY_CHUNK_OVERLAP)
        if not chunks:
            continue
        title  = fp.stem.replace("_", " ").replace("-", " ").strip()
        source = fp.parent.name
        tags   = _guess_tags(fp)

        for i, ch in enumerate(chunks):
            batch_texts.append(ch)
            batch_meta.append({
                "text":        ch,
                "title":       title,
                "source":      source,
                "tags":        tags,
                "chunk_index": i,
                "doc_path":    str(fp.as_posix()),
            })
            total_chunks += 1

            if len(batch_texts) >= BATCH:
                vecs = await _embed(batch_texts)
                await _upsert(col, batch_meta, vecs)
                batch_texts, batch_meta = [], []

    if batch_texts:
        vecs = await _embed(batch_texts)
        await _upsert(col, batch_meta, vecs)

    print(
        f"[security-memory] ingest complete: "
        f"files={len(files)} chunks={total_chunks} collection={SECURITY_COLLECTION}"
    )


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
